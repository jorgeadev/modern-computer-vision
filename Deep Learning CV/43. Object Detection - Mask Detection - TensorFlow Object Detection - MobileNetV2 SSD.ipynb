{"cells":[{"cell_type":"markdown","metadata":{"id":"uQCnYPVDrsgx"},"source":["\n","![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/logo_MCV_W.png)\n","\n","\n","## **Training MobileNetSSD Object Detection on a Mask Dataset**\n","\n","Note! For a most up to date version of this notebook, make sure you copy from:\n","\n","ðŸ’¡ Recommendation: [Open this blog post](https://blog.roboflow.ai/training-a-tensorflow-object-detection-model-with-a-custom-dataset/) to continue.\n","\n","### **Overview**\n","\n","This notebook walks through how to train a MobileNet object detection model using the TensorFlow 1.5 Object Detection API.\n","\n","In this specific example, we'll training an object detection model to recognize cells types: white blood cells, red blood cells and platelets. **To adapt this example to train on your own dataset, you only need to change two lines of code in this notebook.**\n","\n","Everything in this notebook is also hosted on this [GitHub repo](https://github.com/josephofiowa/tensorflow-object-detection).\n","\n","![Mask Detector Output](https://github.com/rajeevratan84/ModernComputerVision/raw/main/mask.png)\n","\n","**Credit to [DLology](https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/) and [Tony607](https://github.com/Tony607)**, whom wrote the first notebook on which much of this is example is based. \n","\n","### **Our Data**\n","\n","We'll be using an open source Mask Wearing Dataset which contains 149 images and is hosted publicly on Roboflow [here](https://public.roboflow.com/object-detection/mask-wearing).\n","\n","When adapting this example to your own data, create two datasets in Roboflow: `train` and `test`. Use Roboflow to generate TFRecords for each, replace their URLs in this notebook, and you're able to train on your own custom dataset.\n","\n","### **Our Model**\n","\n","We'll be training a MobileNetSSDv2 (single shot detector). This specific model is a one-short learner, meaning each image only passes through the network once to make a prediction, which allows the architecture to be very performant for mobile hardware.\n","\n","The model arechitecture is one of many available via TensorFlow's [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models).\n","\n","As a note, this notebook presumes TensorFlow 1.5 as TensorFlow 2.0 has yet to fully support the object detection API.\n","\n","### **Training**\n","\n","Google Colab provides free GPU resources. Click \"Runtime\" â†’ \"Change runtime type\" â†’ Hardware Accelerator dropdown to \"GPU.\"\n","\n","Colab does have memory limitations, and notebooks must be open in your browser to run. Sessions automatically clear themselves after 12 hours.\n","\n","### **Inference**\n","\n","We'll run inference directly in this notebook, and on three test images contained in the \"test\" folder from our GitHub repo. \n","\n","When adapting to your own dataset, you'll need to add test images to the `test` folder located at `tensorflow-object-detection/test`.\n","\n","### **About**\n","\n","[Roboflow](https://roboflow.ai) makes managing, preprocessing, augmenting, and versioning datasets for computer vision seamless.\n","\n","Developers reduce 50% of their boilerplate code when using Roboflow's workflow, automate labelling quality assurance, save training time, and increase model reproducibility.\n","\n","#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yhzxsJb3dpWq"},"source":["## Configs and Hyperparameters\n","\n","Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnNXNQCjdniL"},"outputs":[],"source":["# If you forked the repo, you can replace the link.\n","repo_url = 'https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn'\n","\n","# Number of training steps - 1000 will train very quickly, but more steps will increase accuracy.\n","num_steps = 30000  # 200000 to improve\n","\n","# Number of evaluation steps.\n","num_eval_steps = 50\n","\n","MODELS_CONFIG = {\n","    'ssd_mobilenet_v2': {\n","        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n","        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n","        'batch_size': 12\n","    },\n","    'faster_rcnn_inception_v2': {\n","        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n","        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n","        'batch_size': 12\n","    },\n","    'rfcn_resnet101': {\n","        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n","        'pipeline_file': 'rfcn_resnet101_pets.config',\n","        'batch_size': 8\n","    },    \n","}\n","\n","# Pick the model you want to use\n","# Select a model in `MODELS_CONFIG`.\n","selected_model = 'ssd_mobilenet_v2'\n","\n","# Name of the object detection model to use.\n","MODEL = MODELS_CONFIG[selected_model]['model_name']\n","\n","# Name of the pipline file in tensorflow object detection API.\n","pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n","\n","# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n","batch_size = MODELS_CONFIG[selected_model]['batch_size']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1643048761294,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"DL8oRGC5UgMO","outputId":"d54b31d6-c728-4361-a658-8e940e1a032e"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow 1.x selected.\n"]}],"source":["# use TF 1.x for Object Detection APIs as they are not ported to TF 2.0 yet\n","%tensorflow_version 1.x"]},{"cell_type":"markdown","metadata":{"id":"w4V-XE6kbkc1"},"source":["## Clone the `tensorflow-object-detection` repository or your fork."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8410,"status":"ok","timestamp":1643048769691,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"dxc3DmvLQF3z","outputId":"26786784-b11b-4de7-8347-207148ef4ee3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","Cloning into 'tensorflow-object-detection-faster-rcnn'...\n","remote: Enumerating objects: 885, done.\u001b[K\n","remote: Total 885 (delta 0), reused 0 (delta 0), pack-reused 885\u001b[K\n","Receiving objects: 100% (885/885), 24.83 MiB | 3.91 MiB/s, done.\n","Resolving deltas: 100% (428/428), done.\n","/content/tensorflow-object-detection-faster-rcnn\n","Already up to date.\n"]}],"source":["import os\n","\n","%cd /content\n","\n","repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n","\n","!git clone {repo_url}\n","%cd {repo_dir_path}\n","!git pull"]},{"cell_type":"markdown","metadata":{"id":"bI8__uNS8-ns"},"source":["## Install required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49468,"status":"ok","timestamp":1643048819152,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"ecpHEnka8Kix","outputId":"0bff70c8-f329-43e2-e7bb-2e9f876ec11f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","Collecting tf_slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352 kB 12.4 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.12.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n","E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/l/lxml/python-lxml_4.2.1-1ubuntu0.4_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n","E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/pillow/python-pil_5.1.0-1ubuntu0.6_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n","E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n","Collecting lvis\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.15.0)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (2.8.2)\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.19.5)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from lvis) (3.2.2)\n","Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (3.0.6)\n","Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.7/dist-packages (from lvis) (0.29.26)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.3.2)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis) (4.1.2.30)\n","Installing collected packages: lvis\n","Successfully installed lvis-0.5.3\n","/content/models/research\n"]}],"source":["%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!pip install tf_slim\n","\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","\n","!pip install -q pycocotools\n","\n","!pip install lvis\n","\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","\n","!python object_detection/builders/model_builder_test.py"]},{"cell_type":"markdown","metadata":{"id":"u-k7uGThXlny"},"source":["## Prepare `tfrecord` files\n","\n","Roboflow automatically creates our TFRecord and label_map files that we need!\n","\n","**Generating your own TFRecords the only step you need to change for your own custom dataset.**\n","\n","Because we need one TFRecord file for our training data, and one TFRecord file for our test data, we'll create two separate datasets in Roboflow and generate one set of TFRecords for each.\n","\n","To create a dataset in Roboflow and generate TFRecords, follow [this step-by-step guide](https://blog.roboflow.ai/getting-started-with-roboflow/)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1643048821156,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"GNfIPc5yxDOv","outputId":"ccc85ebd-50b7-46ae-a0ca-b7a75bb916c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/tensorflow-object-detection-faster-rcnn/data\n"]}],"source":["%cd /content/tensorflow-object-detection-faster-rcnn/data"]},{"cell_type":"code","source":["# Download our dataset\n","!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/Mask+Wearing.v4-raw.tensorflow.zip\n","!unzip -q 'Mask Wearing.v4-raw.tensorflow.zip'"],"metadata":{"id":"jYscLBVmc9ke","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646415338501,"user_tz":0,"elapsed":6006,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"}},"outputId":"27330b94-18de-45ea-ed15-d16511b042f2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1awECBil5rjNVI_g0YUT_nLSgFeCMVIYc\n","To: /content/Mask Wearing.v4-raw.tensorflow.zip\n","\r  0% 0.00/19.5M [00:00<?, ?B/s]\r100% 19.4M/19.5M [00:00<00:00, 189MB/s]\r100% 19.5M/19.5M [00:00<00:00, 187MB/s]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1643048822105,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"m6qI1uVamksZ","outputId":"8059186f-6699-48d1-a8d0-f643b6a38b49"},"outputs":[{"name":"stdout","output_type":"stream","text":["People_label_map.pbtxt  People.tfrecord\n"]}],"source":["# training set\n","%ls train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1643048822105,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"zEKOjMejmm9U","outputId":"263b3b2b-bb75-4502-fa65-09629df2aef4"},"outputs":[{"name":"stdout","output_type":"stream","text":["People_label_map.pbtxt  People.tfrecord\n"]}],"source":["# test set\n","%ls test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgd-fzAIkZlV"},"outputs":[],"source":["# NOTE: Update these TFRecord names from \"cells\" and \"cells_label_map\" to your files!\n","test_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord'\n","train_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/People.tfrecord'\n","label_map_pbtxt_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/People_label_map.pbtxt'"]},{"cell_type":"markdown","metadata":{"id":"iCNYAaC7w6N8"},"source":["## Download base model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4058,"status":"ok","timestamp":1643048826159,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"orDCj6ihgUMR","outputId":"454ddd28-c275-4b09-f7cf-8c24ab9f71f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/models/research\n"]}],"source":["%cd /content/models/research\n","\n","import os\n","import shutil\n","import glob\n","import urllib.request\n","import tarfile\n","MODEL_FILE = MODEL + '.tar.gz'\n","DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n","DEST_DIR = '/content/models/research/pretrained_model'\n","\n","if not (os.path.exists(MODEL_FILE)):\n","    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n","\n","tar = tarfile.open(MODEL_FILE)\n","tar.extractall()\n","tar.close()\n","\n","os.remove(MODEL_FILE)\n","if (os.path.exists(DEST_DIR)):\n","    shutil.rmtree(DEST_DIR)\n","os.rename(MODEL, DEST_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":546,"status":"ok","timestamp":1643048826692,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"pGhvAObeiIix","outputId":"d8d88d68-f2df-474d-cea0-e79e3da69e71"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/models/research/pretrained_model\n","total 135M\n","drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n","drwxr-xr-x 23 root   root  4.0K Jan 24 18:27 ..\n","-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n","-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n","-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n","-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n","-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n","-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n","drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"]}],"source":["!echo {DEST_DIR}\n","!ls -alh {DEST_DIR}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":57},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1643048826692,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"UHnxlfRznPP3","outputId":"3025500a-c3e7-41f5-bb9a-654c2f88b181"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/models/research/pretrained_model/model.ckpt'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n","fine_tune_checkpoint"]},{"cell_type":"markdown","metadata":{"id":"MvwtHlLOeRJD"},"source":["## Configuring a Training Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIhw7IdpLuiU"},"outputs":[],"source":["import os\n","pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n","\n","assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fG1nCNpUXcRU"},"outputs":[],"source":["def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjtCbLF2i0wI"},"outputs":[],"source":["import re\n","\n","num_classes = get_num_classes(label_map_pbtxt_fname)\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open(pipeline_fname, 'w') as f:\n","    \n","    # fine_tune_checkpoint\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","    \n","    # tfrecord files train and test.\n","    s = re.sub(\n","        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n","\n","    # label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set training batch_size.\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","    \n","    # Set number of classes num_classes.\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","    f.write(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1643048827840,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"GH0MEEanocn6","outputId":"1d5e56dc-4dec-42d0-c28e-c280e13edd7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n","# Users should configure the fine_tune_checkpoint field in the train config as\n","# well as the label_map_path and input_path fields in the train_input_reader and\n","# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n","# should be configured.\n","\n","model {\n","  ssd {\n","    num_classes: 2\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    anchor_generator {\n","      ssd_anchor_generator {\n","        num_layers: 6\n","        min_scale: 0.2\n","        max_scale: 0.95\n","        aspect_ratios: 1.0\n","        aspect_ratios: 2.0\n","        aspect_ratios: 0.5\n","        aspect_ratios: 3.0\n","        aspect_ratios: 0.3333\n","      }\n","    }\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 300\n","        width: 300\n","      }\n","    }\n","    box_predictor {\n","      convolutional_box_predictor {\n","        min_depth: 0\n","        max_depth: 0\n","        num_layers_before_predictor: 0\n","        use_dropout: false\n","        dropout_keep_probability: 0.8\n","        kernel_size: 1\n","        box_code_size: 4\n","        apply_sigmoid_to_scores: false\n","        conv_hyperparams {\n","          activation: RELU_6,\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.00004\n","            }\n","          }\n","          initializer {\n","            truncated_normal_initializer {\n","              stddev: 0.03\n","              mean: 0.0\n","            }\n","          }\n","          batch_norm {\n","            train: true,\n","            scale: true,\n","            center: true,\n","            decay: 0.9997,\n","            epsilon: 0.001,\n","          }\n","        }\n","      }\n","    }\n","    feature_extractor {\n","      type: 'ssd_mobilenet_v2'\n","      min_depth: 16\n","      depth_multiplier: 1.0\n","      conv_hyperparams {\n","        activation: RELU_6,\n","        regularizer {\n","          l2_regularizer {\n","            weight: 0.00004\n","          }\n","        }\n","        initializer {\n","          truncated_normal_initializer {\n","            stddev: 0.03\n","            mean: 0.0\n","          }\n","        }\n","        batch_norm {\n","          train: true,\n","          scale: true,\n","          center: true,\n","          decay: 0.9997,\n","          epsilon: 0.001,\n","        }\n","      }\n","    }\n","    loss {\n","      classification_loss {\n","        weighted_sigmoid {\n","        }\n","      }\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      hard_example_miner {\n","        num_hard_examples: 3000\n","        iou_threshold: 0.99\n","        loss_type: CLASSIFICATION\n","        max_negatives_per_positive: 3\n","        min_negatives_per_image: 3\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    normalize_loss_by_num_matches: true\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 1e-8\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 100\n","      }\n","      score_converter: SIGMOID\n","    }\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 12\n","  optimizer {\n","    rms_prop_optimizer: {\n","      learning_rate: {\n","        exponential_decay_learning_rate {\n","          initial_learning_rate: 0.004\n","          decay_steps: 800720\n","          decay_factor: 0.95\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","      decay: 0.9\n","      epsilon: 1.0\n","    }\n","  }\n","  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n","  fine_tune_checkpoint_type:  \"detection\"\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 30000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    ssd_random_crop {\n","    }\n","  }\n","}\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/People.tfrecord\"\n","  }\n","  label_map_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/People_label_map.pbtxt\"\n","}\n","\n","eval_config: {\n","  num_examples: 8000\n","  # Note: The below line limits the evaluation process to 10 evaluations.\n","  # Remove the below line to evaluate indefinitely.\n","  max_evals: 10\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord\"\n","  }\n","  label_map_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/People_label_map.pbtxt\"\n","  shuffle: false\n","  num_readers: 1\n","}"]}],"source":["!cat {pipeline_fname}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f11w0uO3jFCB"},"outputs":[],"source":["model_dir = 'training/'\n","# Optionally remove content in output model directory to fresh start.\n","!rm -rf {model_dir}\n","os.makedirs(model_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"23TECXvNezIF"},"source":["## Run Tensorboard(Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6696,"status":"ok","timestamp":1643048835627,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"0H2PZs-mSCmO","outputId":"55fef626-49e9-4bfa-f5c8-e233ebdd8082"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-01-24 18:27:07--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 52.202.168.65, 54.237.133.81, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13832437 (13M) [application/octet-stream]\n","Saving to: â€˜ngrok-stable-linux-amd64.zipâ€™\n","\n","ngrok-stable-linux- 100%[===================>]  13.19M  3.56MB/s    in 6.7s    \n","\n","2022-01-24 18:27:14 (1.98 MB/s) - â€˜ngrok-stable-linux-amd64.zipâ€™ saved [13832437/13832437]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"]}],"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8o6r1o5SC5M"},"outputs":[],"source":["LOG_DIR = model_dir\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ge1OX7gcSC7S"},"outputs":[],"source":["get_ipython().system_raw('./ngrok http 6006 &')"]},{"cell_type":"markdown","metadata":{"id":"m5GSGxZNh8rp"},"source":["### Get Tensorboard link"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1643048836015,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"rjhPT9iPSJ6T","outputId":"16ee2c7f-0ecc-42e5-c512-35e8da17e08a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","IndexError: list index out of range\n"]}],"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""]},{"cell_type":"markdown","metadata":{"id":"JDddx2rPfex9"},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"CjDHjhKQofT5","outputId":"9e8b9890-c5ed-4b3c-9424-61096f2d3caf"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n","W0124 18:27:25.741594 139991155734400 model_lib.py:841] Forced number of epochs for all eval validations to be 1.\n","INFO:tensorflow:Maybe overwriting train_steps: 30000\n","I0124 18:27:25.741883 139991155734400 config_util.py:552] Maybe overwriting train_steps: 30000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I0124 18:27:25.742031 139991155734400 config_util.py:552] Maybe overwriting use_bfloat16: False\n","INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n","I0124 18:27:25.742197 139991155734400 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n","INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n","I0124 18:27:25.742336 139991155734400 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n","WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","W0124 18:27:25.742511 139991155734400 model_lib.py:857] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu None\n","I0124 18:27:25.742668 139991155734400 model_lib.py:894] create_estimator_and_inputs: use_tpu False, export_to_tpu None\n","INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51bc37d690>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","I0124 18:27:25.743323 139991155734400 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51bc37d690>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f51bc436ef0>) includes params argument, but params are not passed to Estimator.\n","W0124 18:27:25.743598 139991155734400 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f51bc436ef0>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Not using Distribute Coordinator.\n","I0124 18:27:25.744279 139991155734400 estimator_training.py:186] Not using Distribute Coordinator.\n","INFO:tensorflow:Running training and evaluation locally (non-distributed).\n","I0124 18:27:25.744542 139991155734400 training.py:612] Running training and evaluation locally (non-distributed).\n","INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","I0124 18:27:25.744901 139991155734400 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","W0124 18:27:25.758056 139991155734400 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/tensorflow-object-detection-faster-rcnn/data/train/People.tfrecord']\n","I0124 18:27:25.784945 139991155734400 dataset_builder.py:163] Reading unweighted datasets: ['/content/tensorflow-object-detection-faster-rcnn/data/train/People.tfrecord']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/tensorflow-object-detection-faster-rcnn/data/train/People.tfrecord']\n","I0124 18:27:25.785628 139991155734400 dataset_builder.py:80] Reading record datasets for input file: ['/content/tensorflow-object-detection-faster-rcnn/data/train/People.tfrecord']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0124 18:27:25.785780 139991155734400 dataset_builder.py:81] Number of filenames to read: 1\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W0124 18:27:25.785905 139991155734400 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","W0124 18:27:25.791674 139991155734400 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W0124 18:27:25.812382 139991155734400 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f51bc31c890>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n","W0124 18:27:25.867545 139991155734400 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f51bc31c890>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n","WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f51bc3283b0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0124 18:27:26.094392 139991155734400 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f51bc3283b0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:111: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0124 18:27:26.095770 139991155734400 deprecation.py:323] From /content/models/research/object_detection/inputs.py:111: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:97: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W0124 18:27:26.111631 139991155734400 deprecation.py:323] From /content/models/research/object_detection/inputs.py:97: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:200: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","W0124 18:27:26.238497 139991155734400 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:200: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:284: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0124 18:27:27.213664 139991155734400 deprecation.py:323] From /content/models/research/object_detection/inputs.py:284: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","INFO:tensorflow:Calling model_fn.\n","I0124 18:27:27.748844 139991155734400 estimator.py:1148] Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0124 18:27:27.969548 139991155734400 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:27:31.042943 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:27:31.078497 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:27:31.113007 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:27:31.147110 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:27:31.180471 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:27:31.222472 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0124 18:27:35.823551 139991155734400 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","INFO:tensorflow:Done calling model_fn.\n","I0124 18:27:42.928846 139991155734400 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","I0124 18:27:42.930237 139991155734400 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","I0124 18:27:46.788960 139991155734400 monitored_session.py:240] Graph was finalized.\n","2022-01-24 18:27:46.800918: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2022-01-24 18:27:46.801246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55afe74512c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2022-01-24 18:27:46.801282: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2022-01-24 18:27:46.805546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2022-01-24 18:27:47.074601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:27:47.075701: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55afe7450f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2022-01-24 18:27:47.075734: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2022-01-24 18:27:47.076957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:27:47.077922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2022-01-24 18:27:47.091645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-24 18:27:47.232753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-01-24 18:27:47.253164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-01-24 18:27:47.269832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-01-24 18:27:47.463921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-01-24 18:27:47.481523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-01-24 18:27:47.762515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-24 18:27:47.762731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:27:47.763765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:27:47.764638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2022-01-24 18:27:47.767725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-24 18:27:47.769701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-01-24 18:27:47.769737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2022-01-24 18:27:47.769765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2022-01-24 18:27:47.771091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:27:47.772139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:27:47.773017: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2022-01-24 18:27:47.773073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Running local_init_op.\n","I0124 18:27:53.623615 139991155734400 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0124 18:27:53.967201 139991155734400 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n","I0124 18:28:04.166306 139991155734400 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n","2022-01-24 18:28:14.813316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-24 18:28:16.832966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","INFO:tensorflow:loss = 15.600721, step = 0\n","I0124 18:28:18.162192 139991155734400 basic_session_run_hooks.py:262] loss = 15.600721, step = 0\n","INFO:tensorflow:global_step/sec: 2.84198\n","I0124 18:28:53.348217 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 2.84198\n","INFO:tensorflow:loss = 8.4048815, step = 100 (35.187 sec)\n","I0124 18:28:53.349293 139991155734400 basic_session_run_hooks.py:260] loss = 8.4048815, step = 100 (35.187 sec)\n","INFO:tensorflow:global_step/sec: 3.1532\n","I0124 18:29:25.062021 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1532\n","INFO:tensorflow:loss = 7.771674, step = 200 (31.714 sec)\n","I0124 18:29:25.063322 139991155734400 basic_session_run_hooks.py:260] loss = 7.771674, step = 200 (31.714 sec)\n","INFO:tensorflow:global_step/sec: 3.12005\n","I0124 18:29:57.112718 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.12005\n","INFO:tensorflow:loss = 7.303272, step = 300 (32.051 sec)\n","I0124 18:29:57.113930 139991155734400 basic_session_run_hooks.py:260] loss = 7.303272, step = 300 (32.051 sec)\n","INFO:tensorflow:global_step/sec: 3.14524\n","I0124 18:30:28.906857 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.14524\n","INFO:tensorflow:loss = 7.0256453, step = 400 (31.794 sec)\n","I0124 18:30:28.907888 139991155734400 basic_session_run_hooks.py:260] loss = 7.0256453, step = 400 (31.794 sec)\n","INFO:tensorflow:global_step/sec: 3.05642\n","I0124 18:31:01.624899 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.05642\n","INFO:tensorflow:loss = 6.5341353, step = 500 (32.718 sec)\n","I0124 18:31:01.626074 139991155734400 basic_session_run_hooks.py:260] loss = 6.5341353, step = 500 (32.718 sec)\n","INFO:tensorflow:global_step/sec: 3.06205\n","I0124 18:31:34.282712 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.06205\n","INFO:tensorflow:loss = 6.7992415, step = 600 (32.658 sec)\n","I0124 18:31:34.283921 139991155734400 basic_session_run_hooks.py:260] loss = 6.7992415, step = 600 (32.658 sec)\n","INFO:tensorflow:global_step/sec: 3.15328\n","I0124 18:32:05.995774 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.15328\n","INFO:tensorflow:loss = 6.4251256, step = 700 (31.713 sec)\n","I0124 18:32:05.997175 139991155734400 basic_session_run_hooks.py:260] loss = 6.4251256, step = 700 (31.713 sec)\n","INFO:tensorflow:global_step/sec: 3.17363\n","I0124 18:32:37.505378 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.17363\n","INFO:tensorflow:loss = 6.6027236, step = 800 (31.510 sec)\n","I0124 18:32:37.506725 139991155734400 basic_session_run_hooks.py:260] loss = 6.6027236, step = 800 (31.510 sec)\n","INFO:tensorflow:global_step/sec: 3.18271\n","I0124 18:33:08.925113 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.18271\n","INFO:tensorflow:loss = 5.769734, step = 900 (31.420 sec)\n","I0124 18:33:08.926355 139991155734400 basic_session_run_hooks.py:260] loss = 5.769734, step = 900 (31.420 sec)\n","INFO:tensorflow:global_step/sec: 3.18619\n","I0124 18:33:40.310593 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.18619\n","INFO:tensorflow:loss = 4.9916663, step = 1000 (31.386 sec)\n","I0124 18:33:40.311895 139991155734400 basic_session_run_hooks.py:260] loss = 4.9916663, step = 1000 (31.386 sec)\n","INFO:tensorflow:global_step/sec: 3.10872\n","I0124 18:34:12.478162 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.10872\n","INFO:tensorflow:loss = 6.533415, step = 1100 (32.167 sec)\n","I0124 18:34:12.479365 139991155734400 basic_session_run_hooks.py:260] loss = 6.533415, step = 1100 (32.167 sec)\n","INFO:tensorflow:global_step/sec: 2.97725\n","I0124 18:34:46.066222 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 2.97725\n","INFO:tensorflow:loss = 5.4132943, step = 1200 (33.588 sec)\n","I0124 18:34:46.067375 139991155734400 basic_session_run_hooks.py:260] loss = 5.4132943, step = 1200 (33.588 sec)\n","INFO:tensorflow:global_step/sec: 3.18484\n","I0124 18:35:17.465024 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.18484\n","INFO:tensorflow:loss = 5.125359, step = 1300 (31.399 sec)\n","I0124 18:35:17.466342 139991155734400 basic_session_run_hooks.py:260] loss = 5.125359, step = 1300 (31.399 sec)\n","INFO:tensorflow:global_step/sec: 3.25215\n","I0124 18:35:48.213935 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.25215\n","INFO:tensorflow:loss = 6.1103907, step = 1400 (30.749 sec)\n","I0124 18:35:48.215009 139991155734400 basic_session_run_hooks.py:260] loss = 6.1103907, step = 1400 (30.749 sec)\n","INFO:tensorflow:global_step/sec: 3.10914\n","I0124 18:36:20.377193 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.10914\n","INFO:tensorflow:loss = 4.8647547, step = 1500 (32.165 sec)\n","I0124 18:36:20.380425 139991155734400 basic_session_run_hooks.py:260] loss = 4.8647547, step = 1500 (32.165 sec)\n","INFO:tensorflow:global_step/sec: 3.1635\n","I0124 18:36:51.987723 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1635\n","INFO:tensorflow:loss = 4.702071, step = 1600 (31.609 sec)\n","I0124 18:36:51.989025 139991155734400 basic_session_run_hooks.py:260] loss = 4.702071, step = 1600 (31.609 sec)\n","INFO:tensorflow:global_step/sec: 3.10526\n","I0124 18:37:24.191109 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.10526\n","INFO:tensorflow:loss = 5.295905, step = 1700 (32.203 sec)\n","I0124 18:37:24.192483 139991155734400 basic_session_run_hooks.py:260] loss = 5.295905, step = 1700 (32.203 sec)\n","INFO:tensorflow:global_step/sec: 3.13701\n","I0124 18:37:56.068655 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.13701\n","INFO:tensorflow:loss = 4.6997967, step = 1800 (31.877 sec)\n","I0124 18:37:56.069913 139991155734400 basic_session_run_hooks.py:260] loss = 4.6997967, step = 1800 (31.877 sec)\n","INFO:tensorflow:Saving checkpoints for 1833 into training/model.ckpt.\n","I0124 18:38:06.528336 139991155734400 basic_session_run_hooks.py:606] Saving checkpoints for 1833 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","I0124 18:38:08.000944 139991155734400 dataset_builder.py:163] Reading unweighted datasets: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","I0124 18:38:08.001691 139991155734400 dataset_builder.py:80] Reading record datasets for input file: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0124 18:38:08.001850 139991155734400 dataset_builder.py:81] Number of filenames to read: 1\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f51a8241490>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n","W0124 18:38:08.070973 139991155734400 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f51a8241490>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n","WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f51b5ee9b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0124 18:38:08.283795 139991155734400 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f51b5ee9b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","INFO:tensorflow:Calling model_fn.\n","I0124 18:38:08.912100 139991155734400 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:38:11.304121 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:38:11.341340 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:38:11.377083 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:38:11.413393 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:38:11.448912 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:38:11.484423 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0124 18:38:12.341272 139991155734400 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","W0124 18:38:12.564247 139991155734400 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","INFO:tensorflow:Done calling model_fn.\n","I0124 18:38:13.165230 139991155734400 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2022-01-24T18:38:13Z\n","I0124 18:38:13.184483 139991155734400 evaluation.py:255] Starting evaluation at 2022-01-24T18:38:13Z\n","INFO:tensorflow:Graph was finalized.\n","I0124 18:38:13.669720 139991155734400 monitored_session.py:240] Graph was finalized.\n","2022-01-24 18:38:13.670805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:38:13.671577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2022-01-24 18:38:13.671688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-24 18:38:13.671734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-01-24 18:38:13.671785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-01-24 18:38:13.671876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-01-24 18:38:13.671948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-01-24 18:38:13.671994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-01-24 18:38:13.672040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-24 18:38:13.672171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:38:13.672873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:38:13.673449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2022-01-24 18:38:13.673534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-01-24 18:38:13.673555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2022-01-24 18:38:13.673575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2022-01-24 18:38:13.673721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:38:13.674462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:38:13.675121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-1833\n","I0124 18:38:13.676383 139991155734400 saver.py:1284] Restoring parameters from training/model.ckpt-1833\n","INFO:tensorflow:Running local_init_op.\n","I0124 18:38:14.561493 139991155734400 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0124 18:38:14.681330 139991155734400 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 15 images.\n","I0124 18:38:18.600080 139986924799744 coco_evaluation.py:293] Performing evaluation on 15 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0124 18:38:18.600717 139986924799744 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.00s)\n","I0124 18:38:18.602572 139986924799744 coco_tools.py:138] DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=0.35s).\n","Accumulating evaluation results...\n","DONE (t=0.04s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.014\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.067\n","INFO:tensorflow:Finished evaluation at 2022-01-24-18:38:19\n","I0124 18:38:19.793092 139991155734400 evaluation.py:275] Finished evaluation at 2022-01-24-18:38:19\n","INFO:tensorflow:Saving dict for global step 1833: DetectionBoxes_Precision/mAP = 8.342716e-05, DetectionBoxes_Precision/mAP (large) = 0.00036197365, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.00031360603, DetectionBoxes_Precision/mAP@.75IOU = 3.4780995e-05, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0005494506, DetectionBoxes_Recall/AR@100 = 0.014285714, DetectionBoxes_Recall/AR@100 (large) = 0.06666667, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 8.470507, Loss/localization_loss = 2.3763058, Loss/regularization_loss = 0.30611366, Loss/total_loss = 11.1529255, global_step = 1833, learning_rate = 0.004, loss = 11.1529255\n","I0124 18:38:19.793485 139991155734400 estimator.py:2049] Saving dict for global step 1833: DetectionBoxes_Precision/mAP = 8.342716e-05, DetectionBoxes_Precision/mAP (large) = 0.00036197365, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.00031360603, DetectionBoxes_Precision/mAP@.75IOU = 3.4780995e-05, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0005494506, DetectionBoxes_Recall/AR@100 = 0.014285714, DetectionBoxes_Recall/AR@100 (large) = 0.06666667, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 8.470507, Loss/localization_loss = 2.3763058, Loss/regularization_loss = 0.30611366, Loss/total_loss = 11.1529255, global_step = 1833, learning_rate = 0.004, loss = 11.1529255\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1833: training/model.ckpt-1833\n","I0124 18:38:20.577455 139991155734400 estimator.py:2109] Saving 'checkpoint_path' summary for global step 1833: training/model.ckpt-1833\n","INFO:tensorflow:global_step/sec: 2.1819\n","I0124 18:38:41.900368 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 2.1819\n","INFO:tensorflow:loss = 5.2252755, step = 1900 (45.832 sec)\n","I0124 18:38:41.901581 139991155734400 basic_session_run_hooks.py:260] loss = 5.2252755, step = 1900 (45.832 sec)\n","INFO:tensorflow:global_step/sec: 3.18874\n","I0124 18:39:13.260615 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.18874\n","INFO:tensorflow:loss = 5.064903, step = 2000 (31.360 sec)\n","I0124 18:39:13.261805 139991155734400 basic_session_run_hooks.py:260] loss = 5.064903, step = 2000 (31.360 sec)\n","INFO:tensorflow:global_step/sec: 3.1724\n","I0124 18:39:44.782451 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1724\n","INFO:tensorflow:loss = 5.178323, step = 2100 (31.522 sec)\n","I0124 18:39:44.783610 139991155734400 basic_session_run_hooks.py:260] loss = 5.178323, step = 2100 (31.522 sec)\n","INFO:tensorflow:global_step/sec: 3.1865\n","I0124 18:40:16.164879 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1865\n","INFO:tensorflow:loss = 5.172403, step = 2200 (31.382 sec)\n","I0124 18:40:16.165972 139991155734400 basic_session_run_hooks.py:260] loss = 5.172403, step = 2200 (31.382 sec)\n","INFO:tensorflow:global_step/sec: 3.08722\n","I0124 18:40:48.556460 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.08722\n","INFO:tensorflow:loss = 4.334139, step = 2300 (32.392 sec)\n","I0124 18:40:48.557801 139991155734400 basic_session_run_hooks.py:260] loss = 4.334139, step = 2300 (32.392 sec)\n","INFO:tensorflow:global_step/sec: 3.17178\n","I0124 18:41:20.084461 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.17178\n","INFO:tensorflow:loss = 4.5995007, step = 2400 (31.528 sec)\n","I0124 18:41:20.085753 139991155734400 basic_session_run_hooks.py:260] loss = 4.5995007, step = 2400 (31.528 sec)\n","INFO:tensorflow:global_step/sec: 3.11269\n","I0124 18:41:52.211105 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.11269\n","INFO:tensorflow:loss = 4.0766635, step = 2500 (32.127 sec)\n","I0124 18:41:52.212555 139991155734400 basic_session_run_hooks.py:260] loss = 4.0766635, step = 2500 (32.127 sec)\n","INFO:tensorflow:global_step/sec: 3.15755\n","I0124 18:42:23.881282 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.15755\n","INFO:tensorflow:loss = 5.277748, step = 2600 (31.670 sec)\n","I0124 18:42:23.882406 139991155734400 basic_session_run_hooks.py:260] loss = 5.277748, step = 2600 (31.670 sec)\n","INFO:tensorflow:global_step/sec: 3.04281\n","I0124 18:42:56.745576 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.04281\n","INFO:tensorflow:loss = 3.8282328, step = 2700 (32.864 sec)\n","I0124 18:42:56.746895 139991155734400 basic_session_run_hooks.py:260] loss = 3.8282328, step = 2700 (32.864 sec)\n","INFO:tensorflow:global_step/sec: 3.1563\n","I0124 18:43:28.428260 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1563\n","INFO:tensorflow:loss = 4.8048234, step = 2800 (31.682 sec)\n","I0124 18:43:28.429365 139991155734400 basic_session_run_hooks.py:260] loss = 4.8048234, step = 2800 (31.682 sec)\n","INFO:tensorflow:global_step/sec: 2.98579\n","I0124 18:44:01.920212 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 2.98579\n","INFO:tensorflow:loss = 4.2275724, step = 2900 (33.492 sec)\n","I0124 18:44:01.921381 139991155734400 basic_session_run_hooks.py:260] loss = 4.2275724, step = 2900 (33.492 sec)\n","INFO:tensorflow:global_step/sec: 3.16734\n","I0124 18:44:33.492443 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.16734\n","INFO:tensorflow:loss = 3.9070787, step = 3000 (31.572 sec)\n","I0124 18:44:33.493629 139991155734400 basic_session_run_hooks.py:260] loss = 3.9070787, step = 3000 (31.572 sec)\n","INFO:tensorflow:global_step/sec: 3.16241\n","I0124 18:45:05.113887 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.16241\n","INFO:tensorflow:loss = 4.2852874, step = 3100 (31.622 sec)\n","I0124 18:45:05.115169 139991155734400 basic_session_run_hooks.py:260] loss = 4.2852874, step = 3100 (31.622 sec)\n","INFO:tensorflow:global_step/sec: 3.25134\n","I0124 18:45:35.870542 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.25134\n","INFO:tensorflow:loss = 4.1402607, step = 3200 (30.757 sec)\n","I0124 18:45:35.871899 139991155734400 basic_session_run_hooks.py:260] loss = 4.1402607, step = 3200 (30.757 sec)\n","INFO:tensorflow:global_step/sec: 3.02441\n","I0124 18:46:08.934889 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.02441\n","INFO:tensorflow:loss = 4.058008, step = 3300 (33.064 sec)\n","I0124 18:46:08.936191 139991155734400 basic_session_run_hooks.py:260] loss = 4.058008, step = 3300 (33.064 sec)\n","INFO:tensorflow:global_step/sec: 3.13626\n","I0124 18:46:40.819956 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.13626\n","INFO:tensorflow:loss = 4.2994723, step = 3400 (31.885 sec)\n","I0124 18:46:40.821145 139991155734400 basic_session_run_hooks.py:260] loss = 4.2994723, step = 3400 (31.885 sec)\n","INFO:tensorflow:global_step/sec: 3.17465\n","I0124 18:47:12.319439 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.17465\n","INFO:tensorflow:loss = 4.0755153, step = 3500 (31.500 sec)\n","I0124 18:47:12.320964 139991155734400 basic_session_run_hooks.py:260] loss = 4.0755153, step = 3500 (31.500 sec)\n","INFO:tensorflow:global_step/sec: 3.1799\n","I0124 18:47:43.766909 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1799\n","INFO:tensorflow:loss = 4.5833807, step = 3600 (31.447 sec)\n","I0124 18:47:43.768044 139991155734400 basic_session_run_hooks.py:260] loss = 4.5833807, step = 3600 (31.447 sec)\n","INFO:tensorflow:Saving checkpoints for 3670 into training/model.ckpt.\n","I0124 18:48:06.707836 139991155734400 basic_session_run_hooks.py:606] Saving checkpoints for 3670 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","I0124 18:48:08.175355 139991155734400 dataset_builder.py:163] Reading unweighted datasets: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","I0124 18:48:08.175940 139991155734400 dataset_builder.py:80] Reading record datasets for input file: ['/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0124 18:48:08.176084 139991155734400 dataset_builder.py:81] Number of filenames to read: 1\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f51a00a8750>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n","W0124 18:48:08.242866 139991155734400 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f51a00a8750>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n","WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f512ad2b560> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0124 18:48:08.462235 139991155734400 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f512ad2b560> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","INFO:tensorflow:Calling model_fn.\n","I0124 18:48:09.085973 139991155734400 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:48:11.520719 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:48:11.555897 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:48:11.591638 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:48:11.627442 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:48:11.663317 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0124 18:48:11.698486 139991155734400 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Done calling model_fn.\n","I0124 18:48:13.373675 139991155734400 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2022-01-24T18:48:13Z\n","I0124 18:48:13.392367 139991155734400 evaluation.py:255] Starting evaluation at 2022-01-24T18:48:13Z\n","INFO:tensorflow:Graph was finalized.\n","I0124 18:48:13.872395 139991155734400 monitored_session.py:240] Graph was finalized.\n","2022-01-24 18:48:13.873207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:48:13.874080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2022-01-24 18:48:13.874405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-24 18:48:13.874455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-01-24 18:48:13.874499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-01-24 18:48:13.874541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-01-24 18:48:13.874583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-01-24 18:48:13.874624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-01-24 18:48:13.874674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-24 18:48:13.874769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:48:13.875438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:48:13.876323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2022-01-24 18:48:13.876397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-01-24 18:48:13.876422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2022-01-24 18:48:13.876446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2022-01-24 18:48:13.876683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:48:13.877488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-24 18:48:13.878121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-3670\n","I0124 18:48:13.879382 139991155734400 saver.py:1284] Restoring parameters from training/model.ckpt-3670\n","INFO:tensorflow:Running local_init_op.\n","I0124 18:48:14.714403 139991155734400 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0124 18:48:14.836729 139991155734400 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 15 images.\n","I0124 18:48:18.134986 139986924799744 coco_evaluation.py:293] Performing evaluation on 15 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0124 18:48:18.135486 139986924799744 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.00s)\n","I0124 18:48:18.136935 139986924799744 coco_tools.py:138] DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=0.31s).\n","Accumulating evaluation results...\n","DONE (t=0.04s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n","INFO:tensorflow:Finished evaluation at 2022-01-24-18:48:19\n","I0124 18:48:19.308036 139991155734400 evaluation.py:275] Finished evaluation at 2022-01-24-18:48:19\n","INFO:tensorflow:Saving dict for global step 3670: DetectionBoxes_Precision/mAP = 8.164908e-05, DetectionBoxes_Precision/mAP (large) = 0.00031823415, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.00028092385, DetectionBoxes_Precision/mAP@.75IOU = 6.279698e-05, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0010989011, DetectionBoxes_Recall/AR@100 = 0.00934066, DetectionBoxes_Recall/AR@100 (large) = 0.043589745, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 9.527753, Loss/localization_loss = 2.357426, Loss/regularization_loss = 0.30752948, Loss/total_loss = 12.192707, global_step = 3670, learning_rate = 0.004, loss = 12.192707\n","I0124 18:48:19.308370 139991155734400 estimator.py:2049] Saving dict for global step 3670: DetectionBoxes_Precision/mAP = 8.164908e-05, DetectionBoxes_Precision/mAP (large) = 0.00031823415, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.00028092385, DetectionBoxes_Precision/mAP@.75IOU = 6.279698e-05, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0010989011, DetectionBoxes_Recall/AR@100 = 0.00934066, DetectionBoxes_Recall/AR@100 (large) = 0.043589745, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 9.527753, Loss/localization_loss = 2.357426, Loss/regularization_loss = 0.30752948, Loss/total_loss = 12.192707, global_step = 3670, learning_rate = 0.004, loss = 12.192707\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3670: training/model.ckpt-3670\n","I0124 18:48:19.312503 139991155734400 estimator.py:2109] Saving 'checkpoint_path' summary for global step 3670: training/model.ckpt-3670\n","INFO:tensorflow:global_step/sec: 2.1874\n","I0124 18:48:29.483221 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 2.1874\n","INFO:tensorflow:loss = 5.1541867, step = 3700 (45.716 sec)\n","I0124 18:48:29.484473 139991155734400 basic_session_run_hooks.py:260] loss = 5.1541867, step = 3700 (45.716 sec)\n","INFO:tensorflow:global_step/sec: 3.24929\n","I0124 18:49:00.259103 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.24929\n","INFO:tensorflow:loss = 4.0877266, step = 3800 (30.776 sec)\n","I0124 18:49:00.260205 139991155734400 basic_session_run_hooks.py:260] loss = 4.0877266, step = 3800 (30.776 sec)\n","INFO:tensorflow:global_step/sec: 3.11457\n","I0124 18:49:32.366234 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.11457\n","INFO:tensorflow:loss = 4.7040215, step = 3900 (32.107 sec)\n","I0124 18:49:32.367344 139991155734400 basic_session_run_hooks.py:260] loss = 4.7040215, step = 3900 (32.107 sec)\n","INFO:tensorflow:global_step/sec: 3.11352\n","I0124 18:50:04.484227 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.11352\n","INFO:tensorflow:loss = 4.1602407, step = 4000 (32.118 sec)\n","I0124 18:50:04.485495 139991155734400 basic_session_run_hooks.py:260] loss = 4.1602407, step = 4000 (32.118 sec)\n","INFO:tensorflow:global_step/sec: 3.14034\n","I0124 18:50:36.327913 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.14034\n","INFO:tensorflow:loss = 3.348595, step = 4100 (31.844 sec)\n","I0124 18:50:36.329403 139991155734400 basic_session_run_hooks.py:260] loss = 3.348595, step = 4100 (31.844 sec)\n","INFO:tensorflow:global_step/sec: 3.0623\n","I0124 18:51:08.983218 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.0623\n","INFO:tensorflow:loss = 3.9038982, step = 4200 (32.655 sec)\n","I0124 18:51:08.984616 139991155734400 basic_session_run_hooks.py:260] loss = 3.9038982, step = 4200 (32.655 sec)\n","INFO:tensorflow:global_step/sec: 3.07237\n","I0124 18:51:41.531292 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.07237\n","INFO:tensorflow:loss = 3.6429021, step = 4300 (32.548 sec)\n","I0124 18:51:41.532541 139991155734400 basic_session_run_hooks.py:260] loss = 3.6429021, step = 4300 (32.548 sec)\n","INFO:tensorflow:global_step/sec: 3.14205\n","I0124 18:52:13.357627 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.14205\n","INFO:tensorflow:loss = 3.478609, step = 4400 (31.826 sec)\n","I0124 18:52:13.358636 139991155734400 basic_session_run_hooks.py:260] loss = 3.478609, step = 4400 (31.826 sec)\n","INFO:tensorflow:global_step/sec: 3.1081\n","I0124 18:52:45.531729 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1081\n","INFO:tensorflow:loss = 3.6591406, step = 4500 (32.174 sec)\n","I0124 18:52:45.532947 139991155734400 basic_session_run_hooks.py:260] loss = 3.6591406, step = 4500 (32.174 sec)\n","INFO:tensorflow:global_step/sec: 3.19121\n","I0124 18:53:16.867678 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.19121\n","INFO:tensorflow:loss = 3.646598, step = 4600 (31.336 sec)\n","I0124 18:53:16.868855 139991155734400 basic_session_run_hooks.py:260] loss = 3.646598, step = 4600 (31.336 sec)\n","INFO:tensorflow:global_step/sec: 3.19582\n","I0124 18:53:48.158563 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.19582\n","INFO:tensorflow:loss = 4.162745, step = 4700 (31.291 sec)\n","I0124 18:53:48.159802 139991155734400 basic_session_run_hooks.py:260] loss = 4.162745, step = 4700 (31.291 sec)\n","INFO:tensorflow:global_step/sec: 3.16924\n","I0124 18:54:19.711908 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.16924\n","INFO:tensorflow:loss = 3.2022462, step = 4800 (31.553 sec)\n","I0124 18:54:19.713247 139991155734400 basic_session_run_hooks.py:260] loss = 3.2022462, step = 4800 (31.553 sec)\n","INFO:tensorflow:global_step/sec: 3.10308\n","I0124 18:54:51.937916 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.10308\n","INFO:tensorflow:loss = 3.4853406, step = 4900 (32.226 sec)\n","I0124 18:54:51.939240 139991155734400 basic_session_run_hooks.py:260] loss = 3.4853406, step = 4900 (32.226 sec)\n","INFO:tensorflow:global_step/sec: 3.01297\n","I0124 18:55:25.127730 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.01297\n","INFO:tensorflow:loss = 3.9644122, step = 5000 (33.190 sec)\n","I0124 18:55:25.128850 139991155734400 basic_session_run_hooks.py:260] loss = 3.9644122, step = 5000 (33.190 sec)\n","INFO:tensorflow:global_step/sec: 3.1815\n","I0124 18:55:56.559447 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.1815\n","INFO:tensorflow:loss = 4.0684757, step = 5100 (31.432 sec)\n","I0124 18:55:56.560744 139991155734400 basic_session_run_hooks.py:260] loss = 4.0684757, step = 5100 (31.432 sec)\n","INFO:tensorflow:global_step/sec: 3.11632\n","I0124 18:56:28.648578 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.11632\n","INFO:tensorflow:loss = 3.6185005, step = 5200 (32.089 sec)\n","I0124 18:56:28.649940 139991155734400 basic_session_run_hooks.py:260] loss = 3.6185005, step = 5200 (32.089 sec)\n","INFO:tensorflow:global_step/sec: 3.16846\n","I0124 18:57:00.209714 139991155734400 basic_session_run_hooks.py:692] global_step/sec: 3.16846\n","INFO:tensorflow:loss = 3.6974282, step = 5300 (31.561 sec)\n","I0124 18:57:00.211024 139991155734400 basic_session_run_hooks.py:260] loss = 3.6974282, step = 5300 (31.561 sec)\n"]}],"source":["!python /content/models/research/object_detection/model_main.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --num_eval_steps={num_eval_steps}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1625347122036,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"KP-tUdtnRybs","outputId":"39494137-c461-476d-8a51-2ce8c23b2d59"},"outputs":[{"name":"stdout","output_type":"stream","text":["checkpoint\n","eval_0\n","events.out.tfevents.1625338778.a05eba5d1ba4\n","export\n","graph.pbtxt\n","model.ckpt-21691.data-00000-of-00001\n","model.ckpt-21691.index\n","model.ckpt-21691.meta\n","model.ckpt-23871.data-00000-of-00001\n","model.ckpt-23871.index\n","model.ckpt-23871.meta\n","model.ckpt-26038.data-00000-of-00001\n","model.ckpt-26038.index\n","model.ckpt-26038.meta\n","model.ckpt-28201.data-00000-of-00001\n","model.ckpt-28201.index\n","model.ckpt-28201.meta\n","model.ckpt-30000.data-00000-of-00001\n","model.ckpt-30000.index\n","model.ckpt-30000.meta\n"]}],"source":["!ls {model_dir}"]},{"cell_type":"markdown","metadata":{"id":"OmSESMetj1sa"},"source":["## Exporting a Trained Inference Graph\n","Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12205,"status":"ok","timestamp":1625347134238,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"DHoP90pUyKSq","outputId":"ee1975c0-51f4-48c6-b49b-3e2bea348095"},"outputs":[{"name":"stdout","output_type":"stream","text":["training/model.ckpt-30000\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0703 21:18:46.966856 140607803893632 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0703 21:18:48.923014 140607803893632 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0703 21:18:48.960043 140607803893632 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0703 21:18:48.995502 140607803893632 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0703 21:18:49.034839 140607803893632 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0703 21:18:49.070841 140607803893632 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0703 21:18:49.106505 140607803893632 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/core/post_processing.py:601: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0703 21:18:49.349991 140607803893632 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:601: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","W0703 21:18:49.698288 140607803893632 deprecation.py:323] From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","W0703 21:18:49.701451 140607803893632 deprecation.py:323] From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","W0703 21:18:49.701968 140607803893632 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","135 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              0\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   name\n","-account_type_regexes       _trainable_variables\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     params\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","param: Number of parameters (in the Variable).\n","\n","Profile:\n","node name | # parameters\n","_TFProfRoot (--/4.59m params)\n","  BoxPredictor_0 (--/12.12k params)\n","    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n","      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n","      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n","    BoxPredictor_0/ClassPredictor (--/5.19k params)\n","      BoxPredictor_0/ClassPredictor/biases (9, 9/9 params)\n","      BoxPredictor_0/ClassPredictor/weights (1x1x576x9, 5.18k/5.18k params)\n","  BoxPredictor_1 (--/53.80k params)\n","    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n","      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n","    BoxPredictor_1/ClassPredictor (--/23.06k params)\n","      BoxPredictor_1/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_1/ClassPredictor/weights (1x1x1280x18, 23.04k/23.04k params)\n","  BoxPredictor_2 (--/21.55k params)\n","    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n","      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n","    BoxPredictor_2/ClassPredictor (--/9.23k params)\n","      BoxPredictor_2/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_2/ClassPredictor/weights (1x1x512x18, 9.22k/9.22k params)\n","  BoxPredictor_3 (--/10.79k params)\n","    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n","      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n","    BoxPredictor_3/ClassPredictor (--/4.63k params)\n","      BoxPredictor_3/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_3/ClassPredictor/weights (1x1x256x18, 4.61k/4.61k params)\n","  BoxPredictor_4 (--/10.79k params)\n","    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n","      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n","    BoxPredictor_4/ClassPredictor (--/4.63k params)\n","      BoxPredictor_4/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_4/ClassPredictor/weights (1x1x256x18, 4.61k/4.61k params)\n","  BoxPredictor_5 (--/5.42k params)\n","    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n","      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n","    BoxPredictor_5/ClassPredictor (--/2.32k params)\n","      BoxPredictor_5/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_5/ClassPredictor/weights (1x1x128x18, 2.30k/2.30k params)\n","  FeatureExtractor (--/4.48m params)\n","    FeatureExtractor/MobilenetV2 (--/4.48m params)\n","      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n","        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n","      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n","        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n","        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n","        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n","      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n","        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n","          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n","      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n","      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n","      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n","      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n","      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n","      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n","      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n","      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n","        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n","\n","======================End of Report==========================\n","135 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              1\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   float_ops\n","-account_type_regexes       .*\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     float_ops\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","flops: Number of float operations. Note: Please read the implementation for the math behind it.\n","\n","Profile:\n","node name | # float_ops\n","_TFProfRoot (--/13.71k flops)\n","  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n","  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n","  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n","  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n","  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n","  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n","  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n","  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n","  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n","  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n","  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n","  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n","  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n","  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n","  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n","  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n","  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n","  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n","  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n","  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n","  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n","  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n","  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n","  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n","  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n","  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n","  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n","  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n","  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n","  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n","  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n","  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n","  Preprocessor/map/while/Less_1 (1/1 flops)\n","  Preprocessor/map/while/Less (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n","  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n","\n","======================End of Report==========================\n","2021-07-03 21:18:51.483397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-07-03 21:18:51.502375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.503000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-07-03 21:18:51.503308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-07-03 21:18:51.504752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-07-03 21:18:51.515348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-07-03 21:18:51.515740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-07-03 21:18:51.517499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-07-03 21:18:51.527386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-07-03 21:18:51.543824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-07-03 21:18:51.544004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.544662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.545207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-07-03 21:18:51.551366: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n","2021-07-03 21:18:51.551561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5576b490bd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-07-03 21:18:51.551590: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-07-03 21:18:51.786415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.787245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5576bddfa000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-07-03 21:18:51.787276: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2021-07-03 21:18:51.787463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.788035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-07-03 21:18:51.788109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-07-03 21:18:51.788132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-07-03 21:18:51.788153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-07-03 21:18:51.788173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-07-03 21:18:51.788196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-07-03 21:18:51.788214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-07-03 21:18:51.788234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-07-03 21:18:51.788303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.788930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.789464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-07-03 21:18:51.789560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-07-03 21:18:51.790732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-07-03 21:18:51.790758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-07-03 21:18:51.790769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-07-03 21:18:51.790885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.791463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:51.791994: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-07-03 21:18:51.792036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-30000\n","I0703 21:18:51.793657 140607803893632 saver.py:1284] Restoring parameters from training/model.ckpt-30000\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","W0703 21:18:53.228160 140607803893632 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","2021-07-03 21:18:53.725337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:53.726004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-07-03 21:18:53.726117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-07-03 21:18:53.726143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-07-03 21:18:53.726168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-07-03 21:18:53.726188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-07-03 21:18:53.726210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-07-03 21:18:53.726232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-07-03 21:18:53.726255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-07-03 21:18:53.726356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:53.726972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:53.727529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-07-03 21:18:53.727572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-07-03 21:18:53.727585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-07-03 21:18:53.727596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-07-03 21:18:53.727695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:53.728296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:53.728830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-30000\n","I0703 21:18:53.729989 140607803893632 saver.py:1284] Restoring parameters from training/model.ckpt-30000\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","W0703 21:18:54.266490 140607803893632 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","W0703 21:18:54.266767 140607803893632 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","INFO:tensorflow:Froze 324 variables.\n","I0703 21:18:54.604569 140607803893632 graph_util_impl.py:334] Froze 324 variables.\n","INFO:tensorflow:Converted 324 variables to const ops.\n","I0703 21:18:54.678696 140607803893632 graph_util_impl.py:394] Converted 324 variables to const ops.\n","2021-07-03 21:18:54.800013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:54.800714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-07-03 21:18:54.800816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-07-03 21:18:54.800845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-07-03 21:18:54.800866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-07-03 21:18:54.800887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-07-03 21:18:54.800913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-07-03 21:18:54.800934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-07-03 21:18:54.800955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-07-03 21:18:54.801047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:54.801783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:54.802320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-07-03 21:18:54.802367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-07-03 21:18:54.802382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-07-03 21:18:54.802392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-07-03 21:18:54.802492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:54.803086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-07-03 21:18:54.803646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","W0703 21:18:55.154218 140607803893632 deprecation.py:323] From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","INFO:tensorflow:No assets to save.\n","I0703 21:18:55.154981 140607803893632 builder_impl.py:640] No assets to save.\n","INFO:tensorflow:No assets to write.\n","I0703 21:18:55.155126 140607803893632 builder_impl.py:460] No assets to write.\n","INFO:tensorflow:SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n","I0703 21:18:55.389081 140607803893632 builder_impl.py:425] SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n","INFO:tensorflow:Writing pipeline config file to ./fine_tuned_model/pipeline.config\n","I0703 21:18:55.410256 140607803893632 config_util.py:254] Writing pipeline config file to ./fine_tuned_model/pipeline.config\n"]}],"source":["import re\n","import numpy as np\n","\n","output_directory = './fine_tuned_model'\n","\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)\n","!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory={output_directory} \\\n","    --trained_checkpoint_prefix={last_model_path}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1625347134239,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"usgBZvkz0nqD","outputId":"15781242-969d-45ad-998c-7d2b92d82a08"},"outputs":[{"name":"stdout","output_type":"stream","text":["checkpoint\t\t\tmodel.ckpt.index  saved_model\n","frozen_inference_graph.pb\tmodel.ckpt.meta\n","model.ckpt.data-00000-of-00001\tpipeline.config\n"]}],"source":["!ls {output_directory}"]},{"cell_type":"markdown","metadata":{"id":"p09AOThWkaQv"},"source":["## Download the model `.pb` file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CnDo1lonKgFr"},"outputs":[],"source":["import os\n","\n","pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n","assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1625347134240,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"lHqWkLBINYoI","outputId":"86ad4947-4eb9-47c1-a6e5-4f8f1a9750f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 19M Jul  3 21:18 /content/models/research/fine_tuned_model/frozen_inference_graph.pb\n"]}],"source":["!ls -alh {pb_fname}"]},{"cell_type":"markdown","metadata":{"id":"FIqnjbWYsuQw"},"source":["### Option1 : upload the `.pb` file to your Google Drive\n","Then download it from your Google Drive to local file system.\n","\n","During this step, you will be prompted to enter the token."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238400,"status":"ok","timestamp":1625347372628,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"hAqyASIJqjae","outputId":"7fb0be4d-61b5-4098-b894-fe9205b4e95b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Uploaded file with ID 1lVvi7wGUYp_dLvWrdFGP3rxyMY7MR1IV\n"]}],"source":["# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once in a notebook.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once in a notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","fname = os.path.basename(pb_fname)\n","# Create & upload a text file.\n","uploaded = drive.CreateFile({'title': fname})\n","uploaded.SetContentFile(pb_fname)\n","uploaded.Upload()\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))"]},{"cell_type":"markdown","metadata":{"id":"2FKFq8RXs6bs"},"source":["### Option2 :  Download the `.pb` file directly to your local file system\n","This method may not be stable when downloading large files like the model `.pb` file. Try **option 1** instead if not working."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1625347372634,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"-bP0iMMnnr77","outputId":"af5f1249-1062-4727-c1a1-e1d46f7ddce6"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_9eb3ed20-c1ce-4d01-ba00-04e687d71d2f\", \"frozen_inference_graph.pb\", 19238239)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["from google.colab import files\n","files.download(pb_fname)"]},{"cell_type":"markdown","metadata":{"id":"MFyCeiBb9BbS"},"source":["### OPTIONAL: Download the `label_map.pbtxt` file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":16},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1625347372634,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"K1TbL6Ox8q6Z","outputId":"85bc5708-e748-48ae-9746-8bd94b247256"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_68642a58-e01f-40a7-af2a-d168420b1e86\", \"People_label_map.pbtxt\", 132)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["from google.colab import files\n","files.download(label_map_pbtxt_fname)"]},{"cell_type":"markdown","metadata":{"id":"iUmAo9foa1xq"},"source":["### OPTIONAL: Download the modified pipline file\n","If you plan to use OpenVINO toolkit to convert the `.pb` file to inference faster on Intel's hardware (CPU/GPU, Movidius, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":16},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1625347372635,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"pql2QpemazE1","outputId":"d66d94f4-c6eb-49b2-b07e-a6dd679d8f5b"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_0aa197f1-285b-4b51-9363-5d4ab23af73d\", \"ssd_mobilenet_v2_coco.config\", 4853)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["files.download(pipeline_fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1AgBj1l0v_W"},"outputs":[],"source":["# !tar cfz fine_tuned_model.tar.gz fine_tuned_model\n","# from google.colab import files\n","# files.download('fine_tuned_model.tar.gz')"]},{"cell_type":"markdown","metadata":{"id":"mz1gX19GlVW7"},"source":["## Run inference test\n","Test with images in repository `tensorflow-object-detection/test` directory.\n","\n","**To test with your own images, you need to place your images inside the `test` directory in this Colab notebook!** More on this below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1625347373080,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"Pzj9A4e5mj5l","outputId":"f6c87824-3baf-4fdd-dba0-7b82e0e96c61"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/content/tensorflow-object-detection-faster-rcnn/data/test/1288126-10255706714jpg_jpg.rf.ed230c79fdbb1fa0650ff4fd32f620ee.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/w1240-p16x9-2019-10-04t075956z_1862636027_rc15d4d49d00_rtrmadp_3_hongkong-protests_jpg.rf.a9a2c2a1cb13c980b339065d2324ea7d.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/1224331650_g_400-w_g_jpg.rf.483a35a2395bf48e96783587a59fe876.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/w1240-p16x9-0e48e0098f6e832f27d8b581b33bbc72b9967a63_jpg.rf.b94576bb6f17f4efb5f582ffd9cf9077.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/w1240-p16x9-fa978043deff83fed485af12d16e39c61398fc30_jpg.rf.e68ac161036e4bffd3538903b28c721e.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/126202-untitled-design-13_jpg.rf.baa3d2e55d469ae5d5d4cd81c4603e1d.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/_annotations.csv', '/content/tensorflow-object-detection-faster-rcnn/data/test/15391513329330sooq10859_jpg.rf.bf9e791012521a572a3a7f2979d0dffe.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/1579924271_jpg.rf.1fea0f43731fbea2876f63135256004f.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/People.tfrecord', '/content/tensorflow-object-detection-faster-rcnn/data/test/People_label_map.pbtxt', '/content/tensorflow-object-detection-faster-rcnn/data/test/phplpE73q_jpg.rf.0ab5de066db2a3791b039388014565fe.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/0_Concern-In-China-As-Mystery-Virus-Spreads_jpg.rf.5633f5fe7a9b926101b7fc16615dfb6a.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/cell.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/RTX7CCFN_jpg.rf.501a9e3af09662fc1fbd288a32904267.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/15391513324714o1n0r10n6_jpg.rf.eb6b6b796ad74b0c9d75011d1020f0af.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/the-first-day-of-wuhan-s-closure-some-people-fled-some-panicked_jpg.rf.51ed69bf8d327d93b429a08581f6dea0.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/r1p00017o8171pnq407_jpg.rf.d21dd91220d3df7763f7d572f3d95863.jpg', '/content/tensorflow-object-detection-faster-rcnn/data/test/shutterstock_1627199179_jpg.rf.350e69105dd1458572a590c3e3ef2538.jpg']\n"]}],"source":["import os\n","import glob\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT = pb_fname\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = label_map_pbtxt_fname\n","\n","# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n","PATH_TO_TEST_IMAGES_DIR =  repo_dir_path + \"/data/test/\"\n","sample_img = 'https://storage.googleapis.com/roboflow-platform-transforms/Ly2DeBzbwsemGd2ReHk4BFxy8683/cf5ed147e4f2675fbabbc9b0db750ecf/transformed.jpg'\n","import urllib.request\n","urllib.request.urlretrieve(sample_img, \n","                           PATH_TO_TEST_IMAGES_DIR + \"cell.jpg\")\n","\n","\n","assert os.path.isfile(pb_fname)\n","assert os.path.isfile(PATH_TO_LABELS)\n","TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n","assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n","print(TEST_IMAGE_PATHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625347373080,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"dNFc5CM3Duav","outputId":"2f832991-bf42-425d-e9ff-b13111dc75f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/models/research/object_detection\n"]}],"source":["%cd /content/models/research/object_detection\n","\n","import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","from object_detection.utils import ops as utils_ops\n","\n","\n","# This is needed to display the images.\n","%matplotlib inline\n","\n","\n","from object_detection.utils import label_map_util\n","\n","from object_detection.utils import visualization_utils as vis_util"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG5YUMdg1Po7"},"outputs":[],"source":["detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(\n","    label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","\n","def load_image_into_numpy_array(image):\n","    (im_width, im_height) = image.size\n","    return np.array(image.getdata()).reshape(\n","        (im_height, im_width, 3)).astype(np.uint8)\n","\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (12, 8)\n","\n","\n","def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IbKIjbY_MRk"},"outputs":[],"source":["# This is needed to display the images.\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZMaHYNe6AharysSdQLou3-d-OI4l7a6x"},"executionInfo":{"elapsed":49081,"status":"ok","timestamp":1625347422433,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"Ah9YKYOX9qrH","outputId":"d849f581-1e1c-43ea-82e1-bdd25567fd87"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for image_path in TEST_IMAGE_PATHS:\n","  try:\n","    image = Image.open(image_path)\n","    print(image_path)\n","    # the array based representation of the image will be used later in order to prepare the\n","    # result image with boxes and labels on it.\n","    image_np = load_image_into_numpy_array(image)\n","    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","    image_np_expanded = np.expand_dims(image_np, axis=0)\n","    # Actual detection.\n","    output_dict = run_inference_for_single_image(image_np, detection_graph)\n","    # Visualization of the results of a detection.\n","    vis_util.visualize_boxes_and_labels_on_image_array(\n","        image_np,\n","        output_dict['detection_boxes'],\n","        output_dict['detection_classes'],\n","        output_dict['detection_scores'],\n","        category_index,\n","        instance_masks=output_dict.get('detection_masks'),\n","        use_normalized_coordinates=True,\n","        line_thickness=8)\n","    plt.figure(figsize=IMAGE_SIZE)\n","    plt.imshow(image_np)\n","    plt.show()\n","  except Exception:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5GLCOS1ZhNh"},"outputs":[],"source":["### Adding your own images to tensorflow-object-detection/data\n","def upload_files():\n","  from google.colab import files\n","  uploaded = files.upload()\n","  for k, v in uploaded.items():\n","    open(k, 'wb').write(v)\n","  return list(uploaded.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"executionInfo":{"elapsed":161188,"status":"ok","timestamp":1625347583615,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":-60},"id":"3Zhw17IifBH0","outputId":"e768706b-d559-4338-99f8-41eecdd1b14b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/tensorflow-object-detection-faster-rcnn/data/test\n"]},{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-d887e7c8-fbd2-49c1-bbc7-38efdc743d03\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d887e7c8-fbd2-49c1-bbc7-38efdc743d03\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"output_type":"display_data"},{"data":{"text/plain":["[]"]},"execution_count":60,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# navigate to correct folder\n","%cd /content/tensorflow-object-detection-faster-rcnn/data/test/\n","\n","# call function to upload\n","upload_files()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqKmcDHjvcAl"},"outputs":[],"source":["while True:\n","  pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0D_qKwRveRg"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"43. Object Detection - Mask Detection - TensorFlow Object Detection - MobileNetV2 SSD.ipynb","provenance":[{"file_id":"1wTMIrJhYsQdq_u7ROOkf0Lu_fsX5Mu8a","timestamp":1625338363252},{"file_id":"https://github.com/Tony607/object_detection_demo/blob/master/tensorflow_object_detection_training_colab.ipynb","timestamp":1581243505514}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}